{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Contrail Detection - Part II - Binary Classification\n",
    "This is part 2 in a short series of notebooks that will aim to tackle the Kaggle competition of predicting the presence of contrails in infrared image bands. **Please see Part I of this notebook where I introduce the problem and go over using UNet.**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7586d1d22998cf72"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Introduction\n",
    "In Part I, we introduced the Kaggle competition of predicting contrails in the sky given infrared image bands. We explained why this detection is important, as well as go over the data in detail.\n",
    "\n",
    "Using the infrared image bands, we converted them into human-interpretable false color images. This is an **image segmentation** task, where a single image is classified at the pixel level. In our case, each pixel is either part of a contrail, or it isn't. \n",
    "\n",
    "To handle this new task, we utilized a special neural network called a **UNet**, which consists of an encoder and a decoder. The encoder processes the input image directly, and encodes the image in a smaller latent dimension space. The output layers for the encoder are usually taken from a pre-trained image classification model. In our case, we chose MobileNet as our backbone. Meanwhile, the decoder upsamples the encoded image until it is the same size as the input image. We use pix2pix layers which primarily consist of transposed 2D convolutions to achieve this.\n",
    "\n",
    "While the performance was satisfactory given enough epochs, we proposed an optimization to this network in the final section. As part of our exploratory data analysis, we examined what percentage of images and pixels actually contained contrails. We discovered that **approximately 70%** of images _do not_ contain contrails at all. Therefore, it could be feasible to implement a two-stage model: The first stage is a traditional binary classification at the image level. This model will predict whether the input image contains contrails **anywhere in the image.** The second stage will be our UNet that was trained in Part I.\n",
    "\n",
    "The combination of these two should eliminate any false positives that might occur in images where there are no contrails, and subsequently improve our Dice coefficient score.\n",
    "\n",
    "We will primarily go over the implementation and integration of the regular binary classification model in this notebook. For details on the UNet and its implementation, please refer to Part I."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11fad8c81e9ff993"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import Packages\n",
    "All packages that we used earlier are applicable here. There are no additions or changes necessary. Additionally, we will keep the seed consistent so the dataset is split the same way."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "85f93f3fd4ee0fb4"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 8128\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from typing import Optional\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchinfo\n",
    "\n",
    "from torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "\n",
    "from torchmetrics import Dice, MeanMetric\n",
    "from torchmetrics.classification import BinaryAccuracy\n",
    "from torchmetrics.functional import dice\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.utilities.types import TRAIN_DATALOADERS, EVAL_DATALOADERS\n",
    "\n",
    "pl.seed_everything(8128)\n",
    "\n",
    "ROOT = '../'\n",
    "DATA_DIR = os.path.join(ROOT, 'data', 'google-research-identify-contrails-reduce-global-warming', 'validation')\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T02:44:59.500249400Z",
     "start_time": "2024-02-21T02:44:35.187877500Z"
    }
   },
   "id": "3c41398702797a89",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "## UNet Definitions\n",
    "Next, we'll fully define the UNet which will comprise the second stage of our model. We will use everything other than the final `LightningModule` definition, as that will be adjusted due to the addition of the binary classification model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c667da5afb9290d9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Function to get the segmented images\n",
    "def get_contrail_image_mask(sample_id):\n",
    "    # Read the 3 bands we need, and extract the target time step\n",
    "    band_11 = np.load(os.path.join(DATA_DIR, sample_id, 'band_11.npy'))[:, :, 5]\n",
    "    band_14 = np.load(os.path.join(DATA_DIR, sample_id, 'band_14.npy'))[:, :, 5]\n",
    "    band_15 = np.load(os.path.join(DATA_DIR, sample_id, 'band_15.npy'))[:, :, 5]\n",
    "    \n",
    "    # Let's save the image size, will be useful later\n",
    "    IMAGE_SIZE = band_11.shape[0]\n",
    "    \n",
    "    # Calculate R, G, and B channels, with the scaling.\n",
    "    # Clip to between 0 and 1 so that we don't get invalid values\n",
    "    red = ((band_15 - band_14 + 4) / (2 + 4)).clip(0, 1)\n",
    "    green = ((band_14 - band_11 + 4) / (5 + 4)).clip(0, 1)\n",
    "    blue = ((band_11 - 243) / (303 - 243)).clip(0, 1)\n",
    "    # Stack them correctly, and transpose so that the channels are list\n",
    "    image = np.stack((red, green, blue), axis=0).transpose((1, 2, 0))\n",
    "    \n",
    "    # Now read the mask, it has an extra singleton channel dimension at the end,\n",
    "    # so get rid of that.\n",
    "    mask = np.load(os.path.join(DATA_DIR, sample_id, 'human_pixel_masks.npy')).squeeze()\n",
    "    \n",
    "    return image, mask\n",
    "\n",
    "# pix2pix upsample layer\n",
    "class Pix2PixUpsample(nn.Module):\n",
    "    def __init__(self, in_chan, out_chan, kernel_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv2d = nn.ConvTranspose2d(in_chan, out_chan, kernel_size, stride=2, padding=1, bias=False)\n",
    "        # Initialize weights with mean 0 and standard deviation 0.02\n",
    "        nn.init.normal_(self.conv2d.weight, mean=0, std=0.02)\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            self.conv2d, \n",
    "            nn.BatchNorm2d(out_chan),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# False color dataset class\n",
    "class GRContrailsFalseColorDataset(Dataset):\n",
    "    def __init__(self, image_dir, sample_ids=None, test=False):\n",
    "        \"\"\"\n",
    "        If sample_ids is None, then all the samples in image_idr will be read.\n",
    "        :param image_dir: The directory with all the images.\n",
    "        :param sample_ids: The list of sample IDs to use. Default None\n",
    "        :param test: Whether this is test data or not. If true, then does not return the mask. Default False.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.image_dir = image_dir\n",
    "        if sample_ids is None:\n",
    "            # Get a list of all the subdirectories in image_dir.\n",
    "            # The first element is the directory itself, so index it out.\n",
    "            self.sample_ids = [os.path.basename(subdir) for subdir, _, _ in os.walk(self.image_dir)][1:]\n",
    "        else:\n",
    "            self.sample_ids = sample_ids\n",
    "        self.test = test\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Just return the length of the sample IDs\n",
    "        return len(self.sample_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample_id = self.sample_ids[idx]\n",
    "        # Read in bands 11, 14, and 15 at the target time stamp\n",
    "        band_11 = np.load(os.path.join(self.image_dir, sample_id, 'band_11.npy'))[:, :, 5]\n",
    "        band_14 = np.load(os.path.join(self.image_dir, sample_id, 'band_14.npy'))[:, :, 5]\n",
    "        band_15 = np.load(os.path.join(self.image_dir, sample_id, 'band_15.npy'))[:, :, 5]\n",
    "        # Calculate R, G, and B channels\n",
    "        red = ((band_15 - band_14 + 4) / (2 + 4)).clip(0, 1)\n",
    "        green = ((band_14 - band_11 + 4) / (5 + 4)).clip(0, 1)\n",
    "        blue = ((band_11 - 243) / (303 - 243)).clip(0, 1)\n",
    "        # Concatenate them to create a false color image.\n",
    "        # Do CHANNELS FIRST ordering (axis=0), the default for PyTorch.\n",
    "        image = torch.from_numpy(np.stack((red, green, blue), axis=0))\n",
    "        # Read in the mask, unless this is for testing.\n",
    "        if not self.test:\n",
    "            mask = np.load(os.path.join(self.image_dir, sample_id, 'human_pixel_masks.npy'))\n",
    "            # Mask is 256 x 256 x 1, do a transpose so both input image and mask are the same shape.\n",
    "            # Also convert to float.\n",
    "            mask = torch.from_numpy(np.transpose(mask, (2, 0, 1))).to(torch.float)\n",
    "            return image, mask\n",
    "        else:\n",
    "            return image\n",
    "\n",
    "class GRContrailDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size: int = 128, num_workers: int = 4, \n",
    "                 pin_memory: bool = True, validation_split: float = 0.2):\n",
    "        super().__init__()\n",
    "        # This method allows all parameters to be in self.hparams\n",
    "        # without defining each one individually.\n",
    "        self.save_hyperparameters()\n",
    "        # Define all dataset objects, initially setting to None\n",
    "        self.train_dataset: Optional[Dataset] = None\n",
    "        self.val_dataset: Optional[Dataset] = None\n",
    "    \n",
    "    def prepare_data(self) -> None:\n",
    "        # Download data here, but we've already done so.\n",
    "        pass\n",
    "    \n",
    "    def setup(self, stage: str) -> None:\n",
    "        # Assign dataset objects here by invoking Dataset class with correct parameters\n",
    "        if not self.train_dataset and not self.val_dataset:\n",
    "            # Apply a train test split on the list of all sample IDs present in validation\n",
    "            all_files = [os.path.basename(subdir) for subdir, _, _ in os.walk(DATA_DIR)][1:]\n",
    "            train_files, val_files = train_test_split(all_files, test_size=self.hparams.validation_split)\n",
    "            # Create the two Dataset objects using each of the file lists\n",
    "            self.train_dataset = GRContrailsFalseColorDataset(DATA_DIR, sample_ids=train_files)\n",
    "            self.val_dataset = GRContrailsFalseColorDataset(DATA_DIR, sample_ids=val_files)\n",
    "    \n",
    "    def train_dataloader(self) -> TRAIN_DATALOADERS:\n",
    "        return DataLoader(self.train_dataset, batch_size=self.hparams.batch_size, shuffle=True,\n",
    "                          num_workers=self.hparams.num_workers, pin_memory=self.hparams.pin_memory)\n",
    "    \n",
    "    def val_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        return DataLoader(self.val_dataset, batch_size=self.hparams.batch_size, shuffle=False,\n",
    "                          num_workers=self.hparams.num_workers, pin_memory=self.hparams.pin_memory)\n",
    "\n",
    "class UMobileNet(nn.Module):\n",
    "    def __init__(self, image_size):\n",
    "        super().__init__()\n",
    "        mobilenet = mobilenet_v2(weights=MobileNet_V2_Weights.DEFAULT)\n",
    "        layers = {\n",
    "            'features.2.conv.0': 'block_1',    # 128 x 128\n",
    "            'features.4.conv.0': 'block_3',    # 64 x 64\n",
    "            'features.7.conv.0': 'block_6',    # 32 x 32\n",
    "            'features.14.conv.0': 'block_13',  # 16 x 16\n",
    "            'features.17.conv.2': 'block_16'   # 8 x 8\n",
    "        }\n",
    "        encoder = create_feature_extractor(mobilenet, return_nodes=layers)\n",
    "        self.encoder = encoder\n",
    "        \n",
    "        self.up_stack = nn.ModuleList([\n",
    "            Pix2PixUpsample(320, 512, 4),        # 8 x 8 ==> 16 x 16\n",
    "            Pix2PixUpsample(576 + 512, 256, 4),  # 16 x 16 ==> 32 x 32\n",
    "            Pix2PixUpsample(192 + 256, 128, 4),  # 32 x 32 ==> 64 x 64\n",
    "            Pix2PixUpsample(144 + 128, 64, 4)    # 64 x 64 ==> 128 x 128\n",
    "        ])\n",
    "\n",
    "\n",
    "        # The final layer is just a simple transpose, with a single output channel.\n",
    "        self.last_conv = nn.ConvTranspose2d(in_channels=96 + 64, out_channels=1, kernel_size=4, stride=2, padding=1)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # Push through encoder\n",
    "        skips = list(self.encoder(x).values())\n",
    "        x = skips[-1]\n",
    "        skips = skips[::-1][1:]\n",
    "        \n",
    "        for up, skip_connection in zip(self.up_stack, skips):\n",
    "            x = up(x)\n",
    "            x = torch.cat([x, skip_connection], dim=1)\n",
    "        x = self.last_conv(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T02:49:24.598597500Z",
     "start_time": "2024-02-21T02:49:24.550065500Z"
    }
   },
   "id": "43d9303946a04f40",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset and DataModule changes\n",
    "Because we need to carry forward information about whether the image contains contrails at all, we need to include a binary flag to the label in addition to the original mask. This is simply an extra output in the return statement. I've marked the main change below."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "244e59993e8b041"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# False color dataset class\n",
    "class GRContrailsFalseColorDataset(Dataset):\n",
    "    def __init__(self, image_dir, sample_ids=None, test=False):\n",
    "        \"\"\"\n",
    "        If sample_ids is None, then all the samples in image_idr will be read.\n",
    "        :param image_dir: The directory with all the images.\n",
    "        :param sample_ids: The list of sample IDs to use. Default None\n",
    "        :param test: Whether this is test data or not. If true, then does not return the mask. Default False.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.image_dir = image_dir\n",
    "        if sample_ids is None:\n",
    "            # Get a list of all the subdirectories in image_dir.\n",
    "            # The first element is the directory itself, so index it out.\n",
    "            self.sample_ids = [os.path.basename(subdir) for subdir, _, _ in os.walk(self.image_dir)][1:]\n",
    "        else:\n",
    "            self.sample_ids = sample_ids\n",
    "        self.test = test\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Just return the length of the sample IDs\n",
    "        return len(self.sample_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample_id = self.sample_ids[idx]\n",
    "        # Read in bands 11, 14, and 15 at the target time stamp\n",
    "        band_11 = np.load(os.path.join(self.image_dir, sample_id, 'band_11.npy'))[:, :, 5]\n",
    "        band_14 = np.load(os.path.join(self.image_dir, sample_id, 'band_14.npy'))[:, :, 5]\n",
    "        band_15 = np.load(os.path.join(self.image_dir, sample_id, 'band_15.npy'))[:, :, 5]\n",
    "        # Calculate R, G, and B channels\n",
    "        red = ((band_15 - band_14 + 4) / (2 + 4)).clip(0, 1)\n",
    "        green = ((band_14 - band_11 + 4) / (5 + 4)).clip(0, 1)\n",
    "        blue = ((band_11 - 243) / (303 - 243)).clip(0, 1)\n",
    "        # Concatenate them to create a false color image.\n",
    "        # Do CHANNELS FIRST ordering (axis=0), the default for PyTorch.\n",
    "        image = torch.from_numpy(np.stack((red, green, blue), axis=0))\n",
    "        # Read in the mask, unless this is for testing.\n",
    "        if not self.test:\n",
    "            mask = np.load(os.path.join(self.image_dir, sample_id, 'human_pixel_masks.npy'))\n",
    "            # Mask is 256 x 256 x 1, do a transpose so both input image and mask are the same shape.\n",
    "            # Also convert to float.\n",
    "            mask = torch.from_numpy(np.transpose(mask, (2, 0, 1))).to(torch.float)\n",
    "            return image, int(torch.any(mask)), mask   #### CHANGE HERE!!\n",
    "        else:\n",
    "            return image"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T02:54:14.794744Z",
     "start_time": "2024-02-21T02:54:14.740497100Z"
    }
   },
   "id": "133562b8a5fcc5b0",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `DataModule` itself won't change because that is mainly `Dataset` output agnostic. What **will** need to be changed is the `LightningModule`, because we have an extra output to worry about and manage.\n",
    "\n",
    "But before we get to that, we need to define our binary classification first. This is just as standard `nn.Module`. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1e7e8632fc95788"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3c36bdf5bf2eca92"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ebc5458a6c35c0b4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
